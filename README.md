<h1 align="center">Hi there, I'm <a href="https://t.me/egoluback" target="_blank">Egor Kokush</a> 
<img src="https://github.com/blackcater/blackcater/raw/main/images/Hi.gif" height="32"/></h1>
<h3 align="center">Computer science student, Intern ML-engineer from Russia ðŸ‡·ðŸ‡º</h3>

[![Kaggle Badge](https://img.shields.io/badge/Kaggle-profile-blue)](https://www.kaggle.com/egoluback)

I'm Kokush Egor, 18 y. o. HSE student and novice ML-engineer from Russia. <br />

Contact me on: [Telegram](https://t.me/egoluback) [VK](https://vk.com/egoluback)

![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=plastic&logo=PyTorch&logoColor=white)
![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=plastic&logo=numpy&logoColor=white)
![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=plastic&logo=pandas&logoColor=white)
![Sklearn](https://img.shields.io/badge/Sklearn-%233F4F75.svg?style=plastic&logo=scikit-learn&logoColor=white)

![Python](https://img.shields.io/badge/python-3670A0?style=plastic&logo=python&logoColor=ffdd54)
![C++](https://img.shields.io/badge/c++-%2300599C.svg?style=plastic&logo=c%2B%2B&logoColor=white)

## My last projects
- "Efficient LLM-based metrics for NLG" research project for AIRI Summer School [ [Presentation](https://docs.google.com/presentation/d/1HNTf9DLWdZIoHxJs9yREJllIuufMsnpHGo9o_seEV_s/edit) ] [ [Repo](https://github.com/Rexhaif/airi_summer_llm_metrics) ]
  - We tried to beat GPT4-based GEMBA metric by fine-tuning LLMs for translation evaluation
  - I implemented LLM Encoder+MLP decoder architecture which got the best quality
- "Multimodality in image2text tasks" research project for 1st year of HSE [ [Poster](https://docs.google.com/presentation/d/1_b7Yyl43Ck0212e8oj2vRu3GoVxk9pfwi9Ok_85I9xk/edit#slide=id.p1) ] [ [Repo](https://github.com/Technolog796/image_captioning) ]
  - We implemented the BLIP-2 architecture and tested it on various configurations
  - We adapted architecture for russian language and achieved tolerable quality
- NTI ML contest, 2021 [ [Repo](https://github.com/Egoluback/nti_ml_20-21) ]
  - I used lots of classic ML algorithms(linear and logistic regression, trees, boostings, etc), web-scrapping for data extraction and grid-search for hyperparams search
  - We achieved one of the best scores in final rating
- Toxic detector bot, pet project [ [Repo](https://github.com/Egoluback/Toxic_Detector) ]
  - I trained CatBoostClassifiers for toxicity prediction using word2vec embeddings
- Other pet-projects
</details>
